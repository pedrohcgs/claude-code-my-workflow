---
title: "User Guide"
subtitle: "What You Can Do with the Clo-Author"
---

# How It Works

You describe what you want in plain English. Claude handles the rest.

| You Do | Happens Automatically |
|--------|----------------------|
| Describe what you want | Claude selects and runs the right skills |
| Approve plans | Orchestrator coordinates agents via dependency graph |
| Review final output | Worker-critic pairs ensure quality |
| Say "commit" when ready | Quality gates enforce score thresholds |

For complex or ambiguous tasks, Claude may ask 3--5 clarifying questions to create a **requirements specification** before planning. Each requirement is tagged MUST/SHOULD/MAY, and each aspect is marked CLEAR/ASSUMED/BLOCKED.

---

# Research Workflow

A typical research project flows through these phases, driven by dependencies (not forced sequence):

```
Phase 1: Discovery
  /interview-me → Research Spec + Domain Profile
  /lit-review → Literature Synthesis (Librarian + Editor)

Phase 2: Strategy
  /find-data → Data Assessment (Explorer + Surveyor)
  /identify → Strategy Memo (Strategist + Econometrician)

Phase 3: Execution
  /data-analysis → Scripts + Output (Coder + Debugger)
  /draft-paper → Paper Sections (Writer + Humanizer)

Phase 4: Peer Review
  /paper-excellence → Weighted Score (4 agents parallel)
  /review-paper → 2 Referees + Editor

Phase 5: Submission
  /target-journal → Journal Recommendations
  /submit → Final Gate (score >= 95)
```

You can enter at any stage. Use `/new-project` for the full pipeline. Each skill produces structured output and saves reports to `quality_reports/`.

## Writing Your Paper

### `/draft-paper [section]`

Dispatches the **Writer** agent. Drafts paper sections with proper economics structure:

- **Introduction**: contribution paragraph within first 2 pages, effect sizes stated, clear identification preview
- **Empirical Strategy**: per-design template (DiD, IV, RDD, etc.) with assumption discussion
- **Results**: proper table/figure references, statistical vs economic significance
- **Notation protocol**: $Y_{it}$, $D_{it}$, $ATT(g,t)$ --- consistent throughout

Anti-hedging rules enforced. **Humanizer pass** automatically strips 24 AI writing patterns.

### `/review-paper [file]`

Dispatches 2 **Referee** agents (blind, parallel) and the **Editor** agent. Simulates peer review with independent reports and editorial decision (Accept/Minor/Major/Reject).

### `/respond-to-referee [report]`

Classifies each referee comment per `revision-protocol.md`:

| Class | Routing | Action |
|-------|---------|--------|
| **NEW ANALYSIS** | → Coder | Flag for user, create analysis task |
| **CLARIFICATION** | → Writer | Draft revised text |
| **DISAGREE** | → User | Draft diplomatic pushback (flagged for review) |
| **MINOR** | → Writer | Draft fix directly |

Produces point-by-point response letter with diplomatic disagreement protocol.

## Running Analysis

### `/data-analysis [dataset]`

Dispatches the **Coder** (implementer) and **Debugger** (code critic). End-to-end analysis: data cleaning, main specification, robustness checks, publication-ready output. Supports R, Stata, Python, and Julia.

### `/econometrics-check [file]`

The centerpiece agent. Dispatches the **Econometrician** for **4 sequential phases** with early stopping:

| Phase | What It Does |
|-------|-------------|
| **1. What's the Claim?** | Identify the design (DiD/IV/RDD/SC/Event Study), estimand, treatment, control. Early stops if no causal claims. |
| **2. Core Design Validity** | Design-specific assumption checks (3--5 critical items) plus mandatory sanity check --- sign, magnitude, dynamics. Early stops on critical issues. |
| **3. Inference** | SE clustering, few-clusters correction, multiple testing, code-theory alignment with package-specific checks. |
| **4. Polish** | Robustness (Oster bounds, placebos, alternative specs), assumption stress test, citation fidelity. Only runs if Phases 2--3 pass. |

**Package-flexible:** Recommends best practices but accepts and validates alternative packages.

### `/find-data [question]`

Dispatches the **Explorer** (data finder) and **Surveyor** (data critic). Searches public, administrative, survey, and novel data sources. Scores feasibility (A/B/C/D) and measurement quality.

### `/identify [question]`

Dispatches the **Strategist** (proposer) and **Econometrician** (critic). Produces strategy memo, pseudo-code, robustness plan, and falsification tests.

## Preparing for Submission

### `/pre-analysis-plan [spec]`

Dispatches the **Strategist** in PAP mode. Drafts in AEA/OSF/EGAP format with outcomes, subgroups, multiple testing corrections, and power calculations.

### `/audit-replication [dir]`

Dispatches the **Verifier** in submission mode (all 10 checks):

1. LaTeX compilation
2. Script execution
3. File integrity
4. Output freshness
5. Package inventory
6. Dependency verification
7. Data provenance
8. End-to-end execution
9. Output cross-reference
10. README completeness (AEA format)

### `/data-deposit`

Dispatches **Coder** (assembly) and **Verifier** (validation). Generates AEA-format README, master script, numbered script order, and deposit checklist.

### `/target-journal [paper]`

Dispatches the **Editor** in journal-selection mode. Produces ranked journal list using domain-profile tiers, formatting requirements, and submission strategy.

### `/submit [journal]`

Final submission gate. Requires aggregate score >= 95 with all components >= 80. Generates cover letter draft and submission checklist.

## Giving a Talk

### `/create-talk [format]`

Dispatches the **Storyteller** (creator) and **Discussant** (critic). Generates Beamer `.tex` talks derived from your paper in 4 formats:

| Format | Slides | Duration | Content |
|--------|--------|----------|---------|
| Job Market | 40--50 | 45--60 min | Full story, all results, mechanism, robustness |
| Seminar | 25--35 | 30--45 min | Motivation, main result, 2 robustness checks |
| Short | 10--15 | 15 min | Question, method, key result, implication |
| Lightning | 3--5 | 5 min | Hook, result, so-what |

Talk scores are **advisory** (non-blocking).

---

# Quality Gates

Every file gets a quality score from 0 to 100, computed as a **weighted aggregate** across components. The orchestrator manages this automatically.

| Score | Gate | Applies To |
|-------|------|------------|
| **95+** | Submission | Aggregate + all components >= 80 |
| **90+** | PR | Weighted aggregate (blocking) |
| **80+** | Commit | Weighted aggregate (blocking) |
| **< 80** | Blocked | Must fix before committing |
| -- | Advisory | Talks (reported, non-blocking) |

### Weighted Aggregate Formula

| Component | Weight | Agent |
|-----------|--------|-------|
| Literature | 10% | Librarian + Editor |
| Data | 10% | Explorer + Surveyor |
| Identification | 25% | Econometrician |
| Code | 15% | Debugger |
| Paper | 25% | Proofreader |
| Polish | 10% | Proofreader (writing subscore) |
| Replication | 5% | Verifier |

Missing components are renormalized. See `scoring-protocol.md` for details.

---

# Exploration Workflow {#sec-exploration}

The `explorations/` folder provides a structured sandbox for experimental work.

**The problem:** Without structure, experimental code scatters across the repository.

**The solution:** All experimental work goes into `explorations/` first:

```
explorations/
├── [active-project]/
│   ├── README.md           # Goal, hypotheses, status
│   ├── R/                  # Code iterations (_v1, _v2)
│   └── output/             # Results
└── ARCHIVE/
    ├── completed_[name]/   # Graduated to production
    └── abandoned_[name]/   # Documented why stopped
```

### Fast-Track vs Plan-First

| Question | Answer | Workflow |
|----------|--------|----------|
| "Will this ship?" | YES | Plan-First (80/100 quality) |
| "Am I testing an idea?" | YES | Fast-Track (60/100 quality) |
| "Does this improve the project?" | NO | Don't build it |

---

# Self-Improvement

## Quick Corrections: [LEARN] Tags

Every correction gets tagged for future reference in MEMORY.md:

```markdown
- [LEARN:notation] T_t = 1{t=2} is deterministic -> use T_i in {1,2}
- [LEARN:citation] Post-LASSO is Belloni (2013), NOT Belloni (2014)
- [LEARN:r-code] Package X: ALWAYS include intercept in design matrix
```

## Skill Extraction: /learn

For discoveries that deserve more than a one-line tag, use `/learn` to create a full skill.

| Situation | Use |
|-----------|-----|
| One-liner fix | `[LEARN:category]` tag in MEMORY.md |
| Multi-step workflow | `/learn` to create full skill |
| Error + root cause + solution | `/learn` if reusable, `[LEARN]` if not |

---

# Customizing for Your Domain

## Domain Profile

The domain profile (`.claude/rules/domain-profile.md`) calibrates all agents to your field. Fill it in manually or use `/interview-me` to populate it interactively:

- **Field & adjacent subfields** --- inferred from your topic
- **Target journals** --- ranked by tier
- **Common data sources** --- datasets typical for your area
- **Common identification strategies** --- designs used in your literature
- **Field conventions** --- estimation quirks, clustering norms
- **Seminal references** --- papers every referee expects you to cite
- **Referee concerns** --- the "gotcha" questions referees always ask

## Creating Custom Skills {#sec-create-skills}

Create a skill when you repeatedly explain the same 3+ step workflow to Claude. Each skill lives in `.claude/skills/[name]/SKILL.md`:

```yaml
---
name: your-skill-name
description: [What it does] + [When to use] + [Key capabilities]
argument-hint: "[brief hint for user]"
allowed-tools: ["Read", "Write", "Edit", "Bash", "Task"]
---
```

**Quick start:** Copy `templates/skill-template.md` to `.claude/skills/your-skill-name/SKILL.md` and customize.

---

# All Skills Reference

## Research Pipeline

| Skill | Agents | What It Does |
|-------|--------|-------------|
| `/new-project [topic]` | All (orchestrated) | Full pipeline: idea → paper |
| `/interview-me [topic]` | — | Interactive Q&A → research spec + domain profile |
| `/lit-review [topic]` | Librarian + Editor | Literature search + synthesis |
| `/find-data [question]` | Explorer + Surveyor | Data discovery + quality assessment |
| `/identify [question]` | Strategist + Econometrician | Design identification strategy |
| `/research-ideation [topic]` | — | Research questions + strategies |

## Analysis & Code

| Skill | Agents | What It Does |
|-------|--------|-------------|
| `/data-analysis [dataset]` | Coder + Debugger | End-to-end analysis |
| `/econometrics-check [file]` | Econometrician | 4-phase causal inference audit |
| `/review-r [file]` | Debugger | Code quality review (standalone) |

## Writing & Polish

| Skill | Agents | What It Does |
|-------|--------|-------------|
| `/draft-paper [section]` | Writer | Paper sections + humanizer pass |
| `/proofread [file]` | Proofreader | 6-category manuscript review |
| `/humanizer [file]` | — | Strip 24 AI writing patterns |
| `/compile-latex [file]` | — | 3-pass XeLaTeX + BibTeX |

## Quality & Review

| Skill | Agents | What It Does |
|-------|--------|-------------|
| `/paper-excellence [file]` | 4 parallel | Multi-agent review + weighted score |
| `/review-paper [file]` | 2 Referees + Editor | Simulated peer review |
| `/validate-bib` | — | Cross-reference citations |

## Submission & Deposit

| Skill | Agents | What It Does |
|-------|--------|-------------|
| `/target-journal [paper]` | Editor | Journal targeting + strategy |
| `/respond-to-referee [report]` | Writer + routing | Point-by-point response |
| `/data-deposit` | Coder + Verifier | AEA replication package |
| `/audit-replication [dir]` | Verifier | 10-check submission audit |
| `/pre-analysis-plan [spec]` | Strategist | Draft PAP (AEA/OSF/EGAP) |
| `/submit [journal]` | Verifier + scoring | Final gate (score >= 95) |

## Presentations

| Skill | Agents | What It Does |
|-------|--------|-------------|
| `/create-talk [format]` | Storyteller + Discussant | Beamer talk (4 formats) |
| `/visual-audit [file]` | — | Slide layout audit |

## Infrastructure

| Skill | What It Does |
|-------|-------------|
| `/commit [msg]` | Stage, commit, PR, merge |
| `/learn` | Extract session discoveries into persistent skills |
| `/context-status` | Show session health and context usage |
| `/deploy` | Build and deploy to GitHub Pages |
| `/journal` | Research journal timeline |

---

# Troubleshooting

### LaTeX Won't Compile

**Symptom:** `xelatex` errors or missing packages.

1. Check you have XeLaTeX installed: `which xelatex`
2. Ensure `TEXINPUTS` includes `Preambles/`: the `/compile-latex` skill handles this
3. Missing package? Install via TeX Live: `tlmgr install [package]`

### Hooks Not Firing

**Symptom:** No context warnings, no verification reminders.

1. Check hooks are configured in `.claude/settings.json`
2. Ensure Python 3 is available: `which python3`
3. Check hook file permissions: `ls -la .claude/hooks/`

### Claude Ignores Rules

**Symptom:** Claude doesn't follow conventions in `.claude/rules/`.

1. Rules use `paths:` frontmatter --- check the path matches your files
2. Too many rules? Claude follows ~150 instructions reliably. Consolidate.
3. Try: "Read `.claude/rules/[rule].md` and follow it for this task"

### Context Lost After Compaction

**Symptom:** Claude forgets what you were working on.

1. Point Claude to the plan: "Read `quality_reports/plans/[latest].md`"
2. Check session log: "Read `quality_reports/session_logs/[latest].md`"
3. The `post-compact-restore.py` hook should print recovery info automatically

### Quality Score Too Low

**Symptom:** Score stuck below 80, can't commit.

1. Run `/paper-excellence` to get detailed issue breakdown
2. Fix critical issues first (they cost the most points)
3. Check per-component scores --- one weak component can drag down the aggregate
